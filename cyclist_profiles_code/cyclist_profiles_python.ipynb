{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cyclist_profiles_python.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJVwCVyywqw-",
        "colab_type": "text"
      },
      "source": [
        "# Cyclist Profile  Link Scrape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAFbWVp-wksG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## install the packages I need for this\n",
        "# !pip install beautifulsoup4\n",
        "# !pip install requests\n",
        "# !pip install html5lib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ItYl0Ekx8ot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import packages\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# start with non-looped method to get used to website\n",
        "url = 'https://www.procyclingstats.com/rankings.php?id=49968&nation=&team=&page=0&prev_id=prev&younger=&older=&limit=200&filter=Filter&morefilters=0'\n",
        "html = requests.get(url).text\n",
        "soup = BeautifulSoup(html, 'html5lib')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmQURH2ey2nN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the ider links from the pcs main page\n",
        "links = []\n",
        "for link in soup.find_all('a'):\n",
        "  links.append(link.get('href'))\n",
        "links2 = pd.DataFrame(links)\n",
        "\n",
        "# rename column to links\n",
        "links2.columns = ['links']\n",
        "\n",
        "# Filter to only rider links\n",
        "links2 = links2[links2['links'].str.contains('rider/')]\n",
        "\n",
        "# start with non-looped method to get used to website - 201 to 400\n",
        "url = 'https://www.procyclingstats.com/rankings.php?id=49968&nation=&team=&page=200&prev_id=prev&younger=&older=&limit=200&filter=Filter&morefilters=0'\n",
        "html = requests.get(url).text\n",
        "soup = BeautifulSoup(html, 'html5lib')\n",
        "\n",
        "# get the ider links from the pcs main page\n",
        "links = []\n",
        "for link in soup.find_all('a'):\n",
        "  links.append(link.get('href'))\n",
        "links3 = pd.DataFrame(links)\n",
        "\n",
        "# rename column to links\n",
        "links3.columns = ['links']\n",
        "\n",
        "# Filter to only rider links\n",
        "links3 = links3[links3['links'].str.contains('rider/')]\n",
        "\n",
        "# start with non-looped method to get used to website - 401 to 600\n",
        "url = 'https://www.procyclingstats.com/rankings.php?id=49968&nation=&team=&page=400&prev_id=prev&younger=&older=&limit=200&filter=Filter&morefilters=0'\n",
        "html = requests.get(url).text\n",
        "soup = BeautifulSoup(html, 'html5lib')\n",
        "\n",
        "# get the ider links from the pcs main page\n",
        "links = []\n",
        "for link in soup.find_all('a'):\n",
        "  links.append(link.get('href'))\n",
        "links4 = pd.DataFrame(links)\n",
        "\n",
        "# rename column to links\n",
        "links4.columns = ['links']\n",
        "\n",
        "# Filter to only rider links\n",
        "links4 = links4[links4['links'].str.contains('rider/')]\n",
        "\n",
        "# start with non-looped method to get used to website - 601 to 800\n",
        "url = 'https://www.procyclingstats.com/rankings.php?id=49968&nation=&team=&page=600&prev_id=prev&younger=&older=&limit=200&filter=Filter&morefilters=0'\n",
        "html = requests.get(url).text\n",
        "soup = BeautifulSoup(html, 'html5lib')\n",
        "\n",
        "# get the ider links from the pcs main page\n",
        "links = []\n",
        "for link in soup.find_all('a'):\n",
        "  links.append(link.get('href'))\n",
        "links5 = pd.DataFrame(links)\n",
        "\n",
        "# rename column to links\n",
        "links5.columns = ['links']\n",
        "\n",
        "# Filter to only rider links\n",
        "links5 = links5[links5['links'].str.contains('rider/')]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZB3QOjzXccg_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# append the data sets and save the file\n",
        "links2 = links2.append(links3, ignore_index = True)\n",
        "links2 = links2.append(links4, ignore_index = True)\n",
        "links2 = links2.append(links5, ignore_index = True)\n",
        "\n",
        "# Save the file\n",
        "links2.to_csv('cyclist_links.csv', index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}